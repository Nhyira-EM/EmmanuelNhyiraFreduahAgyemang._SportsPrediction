# -*- coding: utf-8 -*-
"""EmmanuelNhyira_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oEBfbMIU44LX_Ybx7uhn-l6hyDi195pC

EMMANUEL NHYIRA FREDUAH-AGYEMANG

Importing necessary libraries
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, make_scorer, mean_absolute_error
import pickle
import joblib

"""Data Preparation and Feature Extraction:


"""

df_legacy=pd.read_csv('male_players (legacy).csv')
df_22 = pd.read_csv('players_22.csv')

# making both data sets have the same colums so that manipulating and cleaning is easier
df_legacy = df_legacy[['player_url','short_name','long_name','player_positions','potential','value_eur','wage_eur','age','dob','height_cm','weight_kg','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number','club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','preferred_foot','weak_foot','skill_moves','international_reputation','work_rate','body_type','real_face','release_clause_eur','player_tags','player_traits','pace','shooting','passing','dribbling','defending','physic','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control','movement_acceleration','movement_sprint_speed','movement_agility','movement_reactions','movement_balance','power_shot_power','power_jumping','power_stamina','power_strength','power_long_shots','mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed','ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_face_url','overall']]
df_22 = df_22[ ['player_url','short_name','long_name','player_positions','potential','value_eur','wage_eur','age','dob','height_cm','weight_kg','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number','club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','preferred_foot','weak_foot','skill_moves','international_reputation','work_rate','body_type','real_face','release_clause_eur','player_tags','player_traits','pace','shooting','passing','dribbling','defending','physic','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_short_passing','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control','movement_acceleration','movement_sprint_speed','movement_agility','movement_reactions','movement_balance','power_shot_power','power_jumping','power_stamina','power_strength','power_long_shots','mentality_aggression','mentality_interceptions','mentality_positioning','mentality_vision','mentality_penalties','mentality_composure','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed','ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_face_url','overall']]

df_legacy.describe()

df_legacy.info()

print(df_legacy.shape[1])#checking original number of columns in dataset

#droping useless columns
def drop_useless(df):
    useless_columns = ['player_url','short_name','long_name','dob','club_team_id','club_name','league_name','league_level','club_position','club_jersey_number',
                   'club_loaned_from','nationality_id','nationality_name','nation_team_id','nation_position','nation_jersey_number','real_face','release_clause_eur',
                   'player_tags','player_traits','player_face_url']
    df.drop(useless_columns, axis=1, inplace=True)

drop_useless(df_legacy)

#dropping columns with more than 30% null values
def drop_cols_with_high_null_values(df):
    threshold = 0.3 #setting cutoff threshold at 30%
    threshold_count = int(threshold*len(df))
    print(threshold_count)
    new_df= df.dropna(axis=1,thresh=len(df)-threshold_count)

    l=[]
    l_less=[]
    for i in df.columns:
        if((df[i].isnull().sum())<(0.3*(df.shape[0]))):
            l.append(i)
        else:
            l_less.append(i)
    return new_df

new_legacy = drop_cols_with_high_null_values(df_legacy)

print(new_legacy.shape[1]) #checking if columns were dropped

new_legacy

#Adding numbers in colums with '+' sign and converting to integer type
columns = new_legacy.filter(items=['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
                                    'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
                                    'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'])

def int_convert(col):
    col = str(col)  # Converting column to string
    #splitting at '+' or '-' sign
    if '+' in col:
        parts = col.split('+')
        return int(parts[0]) + int(parts[1])
    elif '-' in col:
        parts = col.split('-')
        return int(parts[0]) - int(parts[1])
    else:
        return int(col)

columns = columns.map(int_convert)

new_legacy.update(columns)
int_columns = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
                'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
                'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']

new_legacy[int_columns] = new_legacy[int_columns].astype(int)

new_legacy#checking if code worked

#Separating the data into numeric and cartegorical values
def extract_numerical_categorical(df):
    # Select numerical variables
    num = df.select_dtypes(include=np.number)

    # Select categorical variables
    cat = df.select_dtypes(include=['object'])

    return num, cat

num, cat = extract_numerical_categorical(new_legacy)


num_var = num
cat_var = cat

num, cat = fill_missing_values(num_var, cat_var)

num_var.info()

cat_var.info()

#FILLING MISSING VALUES
def fill_missing_values(num, cat):
    # Filling missing values in numerical data
    num_missing_values = num.columns[num.isnull().any().tolist()]
    for column in num_missing_values:
        num[column].fillna(num[column].mean(), inplace=True)

    # Filling missing values in categorical data
    cat_missing_values = cat.columns[cat.isnull().any().tolist()]
    for column in cat_missing_values:
        cat[column].fillna(cat[column].mode()[0], inplace=True)

    return num, cat

num, cat = fill_missing_values(num_var, cat_var)

num_var.info()#crosschecking that filling was done

#Encoding the cartegorical values
label_encoder = LabelEncoder()

for column in cat_var:
  cat_var[column] = label_encoder.fit_transform(cat_var[column])

cat_var

cleaned_legacy = pd.concat([num_var, cat_var], axis=1)
cleaned_legacy

"""FEATURE ENGINEERING PROCESS:"""

#Checking feature importance

X = cleaned_legacy.drop('overall',axis=1)
y = cleaned_legacy['overall']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rforrest_classifier = RandomForestClassifier(n_estimators=112, max_depth=12, criterion='entropy')

rforrest_classifier.fit(X_train, y_train)

feature_importance = rforrest_classifier.feature_importances_

for name, score in zip(X.columns, feature_importance):
  print(name,'=', score)

#Putting in ascending order
feature_imp_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_imp_df = feature_imp_df.sort_values(by='Importance', ascending=False)
feature_imp_df

#using correlation matrix
corr_matrix = pd.DataFrame(cleaned_legacy.corr())
corr_matrix['overall'].iloc[0:31].sort_values(ascending = False)

#Selecting features from the top features according to the feature importance and the correlation matrix
imp_features = ['potential', 'movement_reactions', 'passing','wage_eur','value_eur','dribbling']

#Scaling the variables
scaler = StandardScaler()
X = cleaned_legacy[imp_features]
X_scaled = scaler.fit_transform(X)
y = pd.DataFrame(cleaned_legacy['overall'])

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
y_train = y_train.values.ravel()#converting y train to a 1D array

"""TRAINING THE MODEL

Random Forest Regressor with grid search
"""

model = RandomForestRegressor(random_state=42)

model.fit(X_train, y_train)
initial_predictions = model.predict(X_test)
initial_rmse = np.sqrt(mean_squared_error(y_test, initial_predictions))
initial_rmse

#Setting grid parameters
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=4)

grid_search.fit(X_train, y_train)

best_parameters = grid_search.best_params_
print(best_parameters)

best_model = grid_search.best_estimator_
best_model

#Evaluate model with cross_val_score
score = cross_val_score(best_model,X,y,scoring='neg_mean_squared_error', cv=5)

rmse_scores = np.sqrt(-score)
rmse_scores

final_rand_for = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=2, random_state=42)

final_rand_for.fit(X_train,y_train)

final_pred = final_rand_for.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test,final_pred))
print("Mean RMSE:", rmse)

final_rand_for.score(X_test, y_test)

"""Xgboost model"""

xgb_model = xgb.XGBRegressor()

xgb_model.fit(X_train, y_train)
xgb_initial_predictions = xgb_model.predict(X_test)
xgb_initial_rmse = np.sqrt(mean_squared_error(y_test, xgb_initial_predictions))
xgb_initial_rmse

"""Hyperparameter Tuning"""

#Setting grid parameters
param_grid1 = {
    'learning_rate': [0.1, 0.01],
    'n_estimators': [100, 500],
    'max_depth': [3, 5],
    'min_child_weight': [1, 3],
    'gamma': [0, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
}

#Root mean score error scorer
scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)
cv = KFold(n_splits=3, shuffle=True, random_state=42)
xgb_grid_search = GridSearchCV(xgb_model, param_grid=param_grid1, scoring=scorer,cv=cv)

xgb_grid_search.fit(X_train, y_train)

xgb_best_parameters = xgb_grid_search.best_params_

#The best xgb model
xgb_best_model = xgb_grid_search.best_estimator_

xgb_cv_scores = cross_val_score(xgb_best_model, X, y, cv=cv, scoring=scorer)
xgb_rmse_scores = -xgb_cv_scores

print("RMSE Scores for Folds:", xgb_rmse_scores)

#Final xgb model
final_xgb = xgb_best_model

#training the final model
final_xgb.fit(X_train,y_train)

#xgb_prediction
xgb_prediction = final_xgb.predict(X_test)
xgb_prediction

mean_absolute_error(xgb_prediction,y_test)

xgb_mean_rmse = np.mean(xgb_rmse_scores)
print("Mean RMSE:", xgb_mean_rmse)

final_xgb.score(X_test, y_test)

"""Gradient boost"""

gb_model = GradientBoostingRegressor()

gb_model.fit(X_train, y_train)
gb_initial_predictions = gb_model.predict(X_test)
gb_initial_rmse = np.sqrt(mean_squared_error(y_test, gb_initial_predictions))
gb_initial_rmse

"""**Hyperparameter tuning:**"""

parameter_grid = {
    'n_estimators': [100, 150, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
}

#Using from xgboost regressor
gb_scorer = scorer

#Using xgboost's cv
cv = cv

gb_grid_search = GridSearchCV(gb_model, parameter_grid, scoring=gb_scorer, cv=cv)

gb_grid_search.fit(X_train,y_train)

gb_best_parameters = gb_grid_search.best_params_
gb_best_parameters

#The best model for GradientBoost
gb_best_model = gb_grid_search.best_estimator_

#Final GradientBoostingRegressor Model
final_gb_model = gb_best_model

gb_prediction = final_gb_model.predict(X_test)
gb_prediction

gb_cv_scores = cross_val_score(final_gb_model, X, y, cv=cv, scoring=scorer)

gb_rmse_score = -gb_cv_scores
gb_rmse_score

#mean root mean square error for GradientboostingRegression
mean_rmse = np.mean(gb_rmse_score)
print("Mean RMSE:", mean_rmse)

final_gb_model.score(X_test, y_test)

"""Testing with player_22 dataset: GradientBoostingRegressor Model, RandomForestRegressor, Xgboost model

Cleaning the players 22
"""

#Cleaning player_22 dataset
drop_useless(df_22)

new_22 = drop_cols_with_high_null_values(df_22)

print(new_legacy.shape[1])

columns = new_22.filter(items=['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
                                    'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
                                    'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk'])

columns = columns.map(int_convert)
new_22.update(columns)

int_columns = ['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
                'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
                'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']

new_22[int_columns] = new_22[int_columns].astype(int)

num, cat = extract_numerical_categorical(new_22)

num_var = num
cat_var = cat

num, cat = fill_missing_values(num_var, cat_var)

label_encoder = LabelEncoder()

for column in cat_var:
  cat_var[column] = label_encoder.fit_transform(cat_var[column])

cleaned_22 = pd.concat([num_var, cat_var], axis=1)
cleaned_22

features = cleaned_22[imp_features]
features

scaled_feat = scaler.fit_transform(features)

new_data_predgb = pd.DataFrame(final_gb_model.predict(scaled_feat))
new_data_predgb

new_data_predxgb = pd.DataFrame(final_xgb.predict(scaled_feat))
new_data_predxgb

new_data_pred = pd.DataFrame(final_rand_for.predict(scaled_feat))
new_data_pred

cleaned_22['overall']

#Evaluating the model's performance on the new data
ground_truth = cleaned_22['overall']
mean_squared_error(ground_truth, new_data_pred)

prediction_rootmse = np.sqrt(mean_squared_error(ground_truth, new_data_predxgb))
prediction_rootmse

"""SAVING THE TRAINED AND TESTED MODEL:"""

#Saving Model as File
import pickle
filename = 'fifa_model.pkl'
pickle.dump(final_xgb, open(filename , 'wb'))